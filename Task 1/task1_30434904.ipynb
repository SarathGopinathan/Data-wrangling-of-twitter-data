{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Sarath Gopinathan\n",
    "#### Student ID: 30434904\n",
    "\n",
    "Date: 05/09/2020\n",
    "\n",
    "Version: 1.4\n",
    "\n",
    "Environment: Python 3.8.5 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* re ( For regular expression, included in Anaconda Python 3.8.5) \n",
    "* OS ( For reading the files from Folders.)\n",
    "* Langid ( For classification of English Tweets from others.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Reading data](#reading_data)\n",
    "* [Filtering Tweets](#lang_tweets)\n",
    "* [Unique Tweets](#unique_tweets)\n",
    "* [Formatting Tweets](#formatting_tweets)\n",
    "* [Writing to file](#writing)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data <a class=\"anchor\" id=\"reading_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = \"./30434904/\"\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "all_tweets_dict = []\n",
    "        \n",
    "# regex    \n",
    "pattern_text = re.compile(r'(?<=(\"text\":\"))(.*?)(?=(\",\"created_at\")|(\",\"id\")|(\",\"withheld\")|(\"}],)|(\"},))')\n",
    "\n",
    "pattern_created_at = re.compile(r'(?<=(\"created_at\":\"))(.*?)(?=(\",\"text\")|(\",\"id\")|(\",\"withheld\")|(\"}],)|(\"},))')\n",
    "\n",
    "pattern_id = re.compile(r'(?<=(\"id\":\"))(.*?)(?=(\",\"text\")|(\",\"created_at\")|(\",\"withheld\")|(\"}],)|(\"},))')\n",
    "\n",
    "# reading   \n",
    "for i in range(len(dirs)):\n",
    "    \n",
    "    text_list = []\n",
    "    created_at_list = []\n",
    "    id_list = []\n",
    "    \n",
    "    with open(path+dirs[i],'r',encoding=\"utf-8\") as infile:\n",
    "        file = infile.read()\n",
    "        \n",
    "    matches_text = pattern_text.finditer(file)\n",
    "\n",
    "    for match in matches_text:\n",
    "        text_list.append(match.group(2))\n",
    "    \n",
    "    matches_created_at = pattern_created_at.finditer(file)\n",
    "\n",
    "    for match in matches_created_at:\n",
    "        created_at_list.append(match.group(2)[0:10])\n",
    "    \n",
    "    matches_id = pattern_id.finditer(file)\n",
    "\n",
    "    # appending to a list \n",
    "    for match in matches_id:\n",
    "        id_list.append(match.group(2))\n",
    "        \n",
    "    # appending to a list     \n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        input_values = {\"date\": created_at_list[j], \"id\": id_list[j], \"tweet\": text_list[j]}\n",
    "        \n",
    "        # appending to required list \n",
    "        all_tweets_dict.append(input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Tweets(Language) <a class=\"anchor\" id=\"lang_tweets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered_tweets = []\n",
    "\n",
    "for k in range(len(all_tweets_dict)):\n",
    "    \n",
    "        current_tweet = all_tweets_dict[k].get('tweet')\n",
    "        \n",
    "        tw = eval(\"\\\"\"+current_tweet+\"\\\"\").encode(\"utf-16\",\"surrogatepass\").decode(\"utf-16\")\n",
    "        \n",
    "        tweet_language = langid.classify(tw)\n",
    "        \n",
    "        # filtering out all eng tweets\n",
    "        if(tweet_language[0] == 'en'):\n",
    "            \n",
    "            input_unique_values = {\"date\": all_tweets_dict[k].get('date'), \"id\": all_tweets_dict[k].get('id'),\n",
    "                               \"tweet\": current_tweet}\n",
    "        \n",
    "            filtered_tweets.append(input_unique_values)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Tweets(Language) <a class=\"anchor\" id=\"unique_tweets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filtering out all unique tweets\n",
    "unique_tweets = [filtered_tweets[0]]\n",
    "for unique in filtered_tweets:\n",
    "    if unique not in unique_tweets:\n",
    "           unique_tweets.append(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Tweets(Language) <a class=\"anchor\" id=\"formatting_tweets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "date_wise_tweets = []\n",
    "\n",
    "for k in range(len(unique_tweets)):\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    for l in range(len(date_wise_tweets)):\n",
    "    \n",
    "        if(date_wise_tweets[l].get('date') == unique_tweets[k].get('date')):\n",
    "            \n",
    "            current_tweet = unique_tweets[k].get('tweet')\n",
    "        \n",
    "            tw = eval(\"\\\"\"+current_tweet+\"\\\"\").encode(\"utf-16\",\"surrogatepass\").decode(\"utf-16\")\n",
    "            \n",
    "            # converting to xml format\n",
    "            tw = str(tw).replace(\"<\", \"&lt;\")\n",
    "        \n",
    "            tw = str(tw).replace(\">\", \"&gt;\")\n",
    "        \n",
    "            tw = str(tw).replace(\"&\", \"&amp;\")\n",
    "        \n",
    "            tw = str(tw).replace(\"'\", \"&apos;\")\n",
    "        \n",
    "            tw = str(tw).replace('\"', \"&quot;\")\n",
    "            \n",
    "            insert_dict = {\"id\": unique_tweets[k].get('id'), \"tweet\": tw}\n",
    "            \n",
    "            date_wise_tweets[l].get('data').append(insert_dict)\n",
    "            \n",
    "            flag = 1            \n",
    "        \n",
    "    if(flag == 0):\n",
    "        data = []\n",
    "        \n",
    "        current_tweet = unique_tweets[k].get('tweet')\n",
    "        \n",
    "        tw = eval(\"\\\"\"+current_tweet+\"\\\"\").encode(\"utf-16\",\"surrogatepass\").decode(\"utf-16\")\n",
    "        \n",
    "        # converting to xml format\n",
    "        tw = str(tw).replace(\"<\", \"&lt;\")\n",
    "        \n",
    "        tw = str(tw).replace(\">\", \"&gt;\")\n",
    "        \n",
    "        tw = str(tw).replace(\"&\", \"&amp;\")\n",
    "        \n",
    "        tw = str(tw).replace(\"'\", \"&apos;\")\n",
    "        \n",
    "        tw = str(tw).replace('\"', \"&quot;\")\n",
    "\n",
    "        insert_dict = {\"id\": unique_tweets[k].get('id'), \"tweet\": tw}\n",
    "\n",
    "        data.append(insert_dict)\n",
    "\n",
    "        date_insert_dict = {\"date\": unique_tweets[k].get('date'), \"data\": data}\n",
    "        \n",
    "        # appending to final list (required format)\n",
    "        date_wise_tweets.append(date_insert_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to file <a class=\"anchor\" id=\"writing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# writing to file\n",
    "with open(\"./30434904.xml\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<data>\\n\")\n",
    "    \n",
    "    for i in range (len(date_wise_tweets)):\n",
    "        \n",
    "        file.write(\"<tweets date=\\\"\" + date_wise_tweets[i].get('date') + \"\\\">\\n\")\n",
    "        \n",
    "        for j in range(len(date_wise_tweets[i].get('data'))):\n",
    "            \n",
    "            file.write(\"<tweet id=\\\"\" + date_wise_tweets[i].get('data')[j].get('id') + \"\\\">\" + \n",
    "               date_wise_tweets[i].get('data')[j].get('tweet') + \"</tweet>\\n\")\n",
    "            \n",
    "        file.write(\"</tweets>\\n\")\n",
    "    \n",
    "    file.write(\"</data>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
