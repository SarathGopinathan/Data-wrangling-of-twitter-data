{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Sarath Gopinathan\n",
    "#### Student ID: 30434904\n",
    "\n",
    "Date: 05/09/2020\n",
    "\n",
    "Version: 1.4\n",
    "\n",
    "Environment: Python 3.8.5 and Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import langid \n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all the data from sheets and converting it into the required format for processing.\n",
    "\n",
    "\n",
    "Tweets format : \n",
    "\n",
    "[\n",
    "{\n",
    "\"date\":\"2020-03-22\",\n",
    "\"tweet\" : \"fiddling while rome burns, the reboot https://t.co/d0idjzgrh2\"\n",
    "},\n",
    "{\n",
    "\"date\":\"2020-03-22\",\n",
    "\"tweet\" : \"tweet 2\"\n",
    "},\n",
    "{\n",
    "\"date\":\"2020-03-22\",\n",
    "\"tweet\" : \"tweet 3\"\n",
    "},\n",
    "....\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tweets_list = []\n",
    "excel_data = pd.ExcelFile('./30434904.xlsx')\n",
    "for i in range(len(excel_data.sheet_names)):\n",
    "    df = excel_data.parse(i)\n",
    "    # Cleaning   \n",
    "    df.dropna(axis=0,inplace=True,how='all')\n",
    "    df.dropna(axis=1,inplace=True,how='all')\n",
    "    if(df.columns[0]!= 'text'):\n",
    "        df.reset_index(inplace=True)\n",
    "        df.drop(axis=1,columns='index',inplace=True)\n",
    "        df.columns= df.iloc[0]\n",
    "        df.drop(index=0,inplace=True)\n",
    "        \n",
    "    for j in range(len(df.get('text'))-1):\n",
    "            \n",
    "        #removing 0 with space unicode values from tweets. eg: \\u2069\n",
    "        tw = (str(df.get('text')[j+1]).encode('ascii', 'ignore')).decode(\"utf-8\").lower()  \n",
    "        \n",
    "        tweet_language = langid.classify(tw)\n",
    "            \n",
    "        if(tweet_language[0] == 'en'):\n",
    "                \n",
    "            temp = {\"date\": df.get('created_at')[j+1][0:10], \"tweet\": tw}\n",
    "            \n",
    "            tweets_list.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting it into required format.\n",
    "\n",
    "New format : \n",
    "\n",
    "[{\n",
    "\"date\":\"2020-03-22\",\n",
    "\"data\":[\n",
    "{\n",
    "\"tweet\" : \"fiddling while rome burns, the reboot https://t.co/d0idjzgrh2\"\n",
    "},\n",
    "{\n",
    "\"tweet\" : \"tweet 2\"\n",
    "},\n",
    "{\n",
    "\"tweet\" : \"tweet 3\"\n",
    "},\n",
    "....\n",
    "]\n",
    "},\n",
    "{\n",
    "\"date\":\"2020-03-22\",\n",
    "\"data\":[\n",
    "{\n",
    "\"tweet\" : \"fiddling while rome burns, the reboot https://t.co/d0idjzgrh2\"\n",
    "},\n",
    "{\n",
    "\"tweet\" : \"tweet 2\"\n",
    "},\n",
    "{\n",
    "\"tweet\" : \"tweet 3\"\n",
    "},\n",
    "....\n",
    "]\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "date_wise_tweets = []\n",
    "\n",
    "for k in range(len(tweets_list)):\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    for l in range(len(date_wise_tweets)):\n",
    "    \n",
    "        if(date_wise_tweets[l].get('date') == tweets_list[k].get('date')):\n",
    "            \n",
    "            current_tweet = tweets_list[k].get('tweet')\n",
    "            \n",
    "            insert_dict = {\"tweet\": current_tweet}\n",
    "            \n",
    "            date_wise_tweets[l].get('data').append(insert_dict)\n",
    "            \n",
    "            flag = 1            \n",
    "        \n",
    "    if(flag == 0):\n",
    "        data = []\n",
    "        \n",
    "        current_tweet = tweets_list[k].get('tweet')\n",
    "\n",
    "        insert_dict = {\"tweet\": current_tweet}\n",
    "\n",
    "        data.append(insert_dict)\n",
    "\n",
    "        date_insert_dict = {\"date\": tweets_list[k].get('date'), \"data\": data}\n",
    "\n",
    "        date_wise_tweets.append(date_insert_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "date_wise_tokenisation = []\n",
    "\n",
    "for i in range(len(date_wise_tweets)):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for j in range(len(date_wise_tweets[i].get('data'))):\n",
    "        \n",
    "        tokens.extend(tokenizer.tokenize(date_wise_tweets[i].get('data')[j].get('tweet')))\n",
    "    \n",
    "    add_dict = {\"date\" : date_wise_tweets[i].get('date'), \"tokens\" : tokens}\n",
    "    \n",
    "    date_wise_tokenisation.append(add_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal\n",
    "\n",
    "date_wise_after_stop_list = []\n",
    "\n",
    "stopwords_df = pd.read_csv('./stopwords_en.txt',names=['stop_words'])\n",
    "stopwords_list = set(stopwords_df['stop_words'])\n",
    "\n",
    "for i in range(len(date_wise_tokenisation)):\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for j in range (len(date_wise_tokenisation[i].get('tokens'))):\n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        if date_wise_tokenisation[i].get('tokens')[j].lower() not in stopwords_list:\n",
    "            \n",
    "            new_tokens.append(date_wise_tokenisation[i].get('tokens')[j].lower())\n",
    "    \n",
    "    add_dict = {\"date\": date_wise_tokenisation[i].get('date'), \"tokens\" : new_tokens}\n",
    "    \n",
    "    date_wise_after_stop_list.append(add_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "final_tokens =[]\n",
    "\n",
    "\n",
    "for i in range(len(date_wise_after_stop_list)):\n",
    "    \n",
    "    stemmed_list = []\n",
    "    \n",
    "    for j in range(len(date_wise_after_stop_list[i].get('tokens'))):\n",
    "\n",
    "#         print(stemmer.stem(pos_list[i].get('tokens')[j][0]))\n",
    "        \n",
    "        stemmed_list.append(stemmer.stem(date_wise_after_stop_list[i].get('tokens')[j]))\n",
    "\n",
    "    add_dict = {\"date\" : date_wise_after_stop_list[i].get('date'), \"tokens\" : stemmed_list}\n",
    "\n",
    "    final_tokens.append(add_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_tokens)):\n",
    "    \n",
    "    less_frequent=FreqDist(final_tokens[i].get('tokens'))\n",
    "    \n",
    "    lessFreqWords = set(less_frequent.hapaxes())     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finiding threshhold words and removing them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_freq_tokens = []\n",
    "\n",
    "for i in range(len(final_tokens)):\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for j in range (len(final_tokens[i].get('tokens'))):\n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        if final_tokens[i].get('tokens')[j].lower() not in lessFreqWords:\n",
    "            \n",
    "            new_tokens.append(final_tokens[i].get('tokens')[j].lower())\n",
    "    \n",
    "    add_dict = {\"date\": final_tokens[i].get('date'), \"tokens\" : new_tokens}\n",
    "    \n",
    "    after_freq_tokens.append(add_dict)\n",
    "\n",
    "# print(date_wise_after_stop_list[1].get('date'))    \n",
    "print(len(after_freq_tokens[0].get('tokens')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "\n",
    "for i in range(len(after_freq_tokens)):\n",
    "    \n",
    "    all_tokens.extend(set(after_freq_tokens[i].get('tokens')))\n",
    " \n",
    "tokens_fd = FreqDist(all_tokens)\n",
    "\n",
    "most_common_tokens = tokens_fd.most_common()\n",
    "\n",
    "threshold_list = []\n",
    "\n",
    "for j in range(len(most_common_tokens)):\n",
    "    \n",
    "    if(most_common_tokens[j][1] > 60):\n",
    "        \n",
    "        threshold_list.append(most_common_tokens[j][0])\n",
    "        \n",
    "threshold_set = set(threshold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datewise_list = []\n",
    "\n",
    "for i in range(len(after_freq_tokens)):\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for j in range (len(after_freq_tokens[i].get('tokens'))):\n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        if after_freq_tokens[i].get('tokens')[j] not in threshold_set:\n",
    "            \n",
    "            new_tokens.append(after_freq_tokens[i].get('tokens')[j])\n",
    "    \n",
    "    add_dict = {\"date\": after_freq_tokens[i].get('date'), \"tokens\" : new_tokens}\n",
    "    \n",
    "    final_datewise_list.append(add_dict)\n",
    "\n",
    "# print(date_wise_after_stop_list[1].get('date'))    \n",
    "# print(len(final_datewise_list[0].get('tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100 uni-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 uni-grams.\n",
    "\n",
    "top_100_uni_list = []\n",
    "\n",
    "for i in range(len(final_datewise_list)):\n",
    "    \n",
    "    freq_dist = FreqDist(final_datewise_list[i].get('tokens'))\n",
    "    \n",
    "    dict = {'date': final_datewise_list[i].get('date'), 'tokens' : freq_dist.most_common(100)}\n",
    "    \n",
    "    top_100_uni_list.append(dict)\n",
    "\n",
    "# print(top_100_uni_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100 bi-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_bi_list = []\n",
    "\n",
    "for i in range(len(date_wise_tokenisation)):\n",
    "    \n",
    "    freq_dist = FreqDist(ngrams(date_wise_tokenisation[i].get('tokens'), n = 2))\n",
    "    \n",
    "    dict = {'date': date_wise_tokenisation[i].get('date'), 'tokens' : freq_dist.most_common(100)}\n",
    "    \n",
    "    top_100_bi_list.append(dict)\n",
    "\n",
    "# print(top_100_bi_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./30434904_100uni.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    for i in range(len(top_100_uni_list)):\n",
    "        \n",
    "        file.writelines(top_100_uni_list[i].get('date') + \":\" + str(top_100_uni_list[i].get('tokens')) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./30434904_100bi.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    for i in range(len(top_100_bi_list)):\n",
    "        \n",
    "        file.writelines(top_100_bi_list[i].get('date') + \":\" + str(top_100_bi_list[i].get('tokens')) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "for i in range(len(final_datewise_list)):\n",
    "    \n",
    "    all_tokens.extend(set(final_datewise_list[i].get('tokens')))\n",
    "\n",
    "tokens_fd = FreqDist(all_tokens)\n",
    "\n",
    "most_common_tokens = tokens_fd.most_common()\n",
    "\n",
    "threshold_list = []\n",
    "\n",
    "for j in range(len(most_common_tokens)):\n",
    "    \n",
    "    if(most_common_tokens[j][1] < 5):\n",
    "        \n",
    "        threshold_list.append(most_common_tokens[j][0])\n",
    "        \n",
    "threshold_set = set(threshold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_threshold_list = []\n",
    "\n",
    "for i in range(len(final_datewise_list)):\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for j in range (len(final_datewise_list[i].get('tokens'))):\n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        if final_datewise_list[i].get('tokens')[j] not in threshold_set:\n",
    "            \n",
    "            new_tokens.append(final_datewise_list[i].get('tokens')[j])\n",
    "    \n",
    "    add_dict = {\"date\": final_datewise_list[i].get('date'), \"tokens\" : new_tokens}\n",
    "    \n",
    "    after_threshold_list.append(add_dict)\n",
    "\n",
    "# print(date_wise_after_stop_list[1].get('date'))    \n",
    "print(len(after_threshold_list[0].get('tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab_list=[]\n",
    "\n",
    "for i in range(len(after_threshold_list)):\n",
    "    final_vocab_list.extend(after_threshold_list[i].get('tokens'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab_list=sorted(set(final_vocab_list))\n",
    "len(final_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "\n",
    "for i in range(len(after_threshold_list)):\n",
    "    list_words.extend(after_threshold_list[i].get('tokens'))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(list_words)\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w)<3)# length less than 3 is i\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi,200) # Top-200 bigrams\n",
    "top_200_bigrams=sorted(top_200_bigrams)\n",
    "\n",
    "print(top_200_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = set()\n",
    "\n",
    "for i in range(len(top_200_bigrams)):\n",
    "    bigram_words = top_200_bigrams[i][0] + \"_\" + top_200_bigrams[i][1]\n",
    "    final_vocab_list.append(bigram_words)\n",
    "\n",
    "final_vocab = sorted(set(final_vocab_list))\n",
    "print(final_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./30434904_vocab.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    for i in range(len(final_vocab)):\n",
    "        \n",
    "        file.writelines(final_vocab[i]+ \":\" + str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_list = []\n",
    "for i in range(len(after_threshold_list)):\n",
    "     \n",
    "    freqdist = FreqDist(after_threshold_list[i].get('tokens'))\n",
    "    \n",
    "    write_dict = {\"date\": after_threshold_list[i].get('date'), \"tokens\" : freqdist.most_common()}\n",
    "    count_vec_list.append(write_dict)\n",
    "\n",
    "print(count_vec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./30434904_countVec.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    for i in range(len(count_vec_list)):\n",
    "        \n",
    "        file.writelines(count_vec_list[i].get('date') + \",\")\n",
    "\n",
    "        for j in range(len(count_vec_list[i].get('tokens'))):\n",
    "\n",
    "            for k in range(len(final_vocab)):\n",
    "\n",
    "                if(count_vec_list[i].get('tokens')[j][0] == final_vocab[k]):\n",
    "\n",
    "                    file.writelines(str(k) +\":\" + str(count_vec_list[i].get('tokens')[j][1]) + \",\")\n",
    "        \n",
    "        file.writelines(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
